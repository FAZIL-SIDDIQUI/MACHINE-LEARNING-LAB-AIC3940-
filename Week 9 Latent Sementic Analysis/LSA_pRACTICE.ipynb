{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 1: \"Cats like milk.\"\n",
    "Document 2: \"Dogs like bones.\"\n",
    "Document 3: \"Cats and dogs are pets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\FAIZ\n",
      "[nltk_data]     SIDDIQUI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\FAIZ\n",
      "[nltk_data]     SIDDIQUI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Imports: We import libraries for text processing (re), data handling, and matrix operations.\n",
    "# Downloads: We download NLTKâ€™s stopwords and wordnet for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define rmv_emails_websites Function\n",
    "# python\n",
    "# Copy code\n",
    "\n",
    "def rmv_emails_websites(string, remove_special_chars=False):\n",
    "    new_str = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", '', string)\n",
    "    new_str = re.sub(r\"https?://\\S+|www\\.\\S+\", '', new_str)\n",
    "    new_str = re.sub(r\"\\b\\S+\\.(com|org|net|edu|gov|co|info|biz|io)\\b\", '', new_str)\n",
    "    new_str = re.sub(r\"[0-9]+\", '', new_str)\n",
    "    \n",
    "    if remove_special_chars:\n",
    "        new_str = re.sub(r\"[^A-Za-z\\s]\", '', new_str)\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "# This function removes email addresses, URLs, and numbers from text. In our small example corpus,\n",
    "# there are no such elements, so this step would simply return the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Preprocessing Function\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = rmv_emails_websites(text)  # Remove unwanted elements\n",
    "    text = text.lower()               # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords_set]\n",
    "    return words\n",
    "\n",
    "# Lowercase Conversion: \"Cats like milk.\" becomes \"cats like milk.\"\n",
    "# Remove Punctuation: \"cats like milk.\"\n",
    "# Remove Stopwords: Removes common words like \"and,\" \"are.\"\n",
    "# Lemmatization: Reduces words to their base form (e.g., \"dogs\" to \"dog\").\n",
    "# Example Output for each document:\n",
    "\n",
    "# Document 1: ['cat', 'like', 'milk']\n",
    "# Document 2: ['dog', 'like', 'bone']\n",
    "# Document 3: ['cat', 'dog', 'pet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Document-Term Matrix (DTM)\n",
    "corpus = [\"Cats like milk.\", \"Dogs like bones.\", \"Cats and dogs are pets.\"]  # Sample corpus\n",
    "preprocessed_corpus = [preprocess(doc) for doc in corpus]\n",
    "\n",
    "# After preprocessing, preprocessed_corpus becomes:\n",
    "# [['cat', 'like', 'milk'], ['dog', 'like', 'bone'], ['cat', 'dog', 'pet']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build Vocabulary and DTM\n",
    "\n",
    "\n",
    "\n",
    "vocab = sorted(set(word for doc in preprocessed_corpus for word in doc))\n",
    "vocab_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Build the DTM\n",
    "dtm = [[0] * len(vocab) for _ in range(len(preprocessed_corpus))]\n",
    "for doc_idx, doc in enumerate(preprocessed_corpus):\n",
    "    word_counts = Counter(doc)\n",
    "    for word, count in word_counts.items():\n",
    "        dtm[doc_idx][vocab_to_index[word]] = count\n",
    "        \n",
    "        \n",
    "# Vocabulary: vocab = ['bone', 'cat', 'dog', 'like', 'milk', 'pet']\n",
    "# Vocabulary Index Mapping:\n",
    "# vocab_to_index = {'bone': 0, 'cat': 1, 'dog': 2, 'like': 3, 'milk': 4, 'pet': 5}  \n",
    "# Document-Term Matrix (DTM):\n",
    "      \n",
    "#       dtm = [\n",
    "#     [0, 1, 0, 1, 1, 0],  # Document 1: \"cat like milk\"\n",
    "#     [1, 0, 1, 1, 0, 0],  # Document 2: \"dog like bone\"\n",
    "#     [0, 1, 1, 0, 0, 1]   # Document 3: \"cat dog pet\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U, singular_values, V\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     17\u001b[0m num_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 18\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m svd_from_scratch(dtm, num_components)\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36msvd_from_scratch\u001b[1;34m(matrix, num_components)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd_from_scratch\u001b[39m(matrix, num_components):\n\u001b[1;32m----> 3\u001b[0m     M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(matrix, matrix\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m      4\u001b[0m     N \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(matrix\u001b[38;5;241m.\u001b[39mT, matrix)\n\u001b[0;32m      5\u001b[0m     eigvals_u, U \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigh(M)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# Step 6: Perform SVD on DTM\n",
    "def svd_from_scratch(matrix, num_components):\n",
    "    M = np.dot(matrix, matrix.T)\n",
    "    N = np.dot(matrix.T, matrix)\n",
    "    eigvals_u, U = np.linalg.eigh(M)\n",
    "    eigvals_v, V = np.linalg.eigh(N)\n",
    "    \n",
    "    idx_u = np.argsort(eigvals_u)[::-1][:num_components]\n",
    "    idx_v = np.argsort(eigvals_v)[::-1][:num_components]\n",
    "    \n",
    "    U = U[:, idx_u]\n",
    "    V = V[:, idx_v]\n",
    "    singular_values = np.sqrt(eigvals_u[idx_u])\n",
    "    \n",
    "    return U, singular_values, V.T\n",
    "\n",
    "num_components = 2\n",
    "U, S, Vt = svd_from_scratch(dtm, num_components)\n",
    "\n",
    "\n",
    "\n",
    "# SVD Calculation: Decomposes the DTM to identify U, S, and Vt, with num_components = 2 as the number of latent topics.\n",
    "# Example Output:\n",
    "# U: Document-topic relationships.\n",
    "# S: Singular values indicating the importance of each topic.\n",
    "# Vt: Term-topic relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Normalize Document-Topic and Topic-Word Matrices\n",
    "\n",
    "document_topic_matrix = U * S[:num_components]\n",
    "topic_word_matrix = Vt[:num_components, :]\n",
    "\n",
    "document_topic_probs = document_topic_matrix / document_topic_matrix.sum(axis=1, keepdims=True)\n",
    "topic_word_probs = topic_word_matrix / topic_word_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"Document-Topic Probabilities:\\n\", document_topic_probs)\n",
    "print(\"Topic-Word Probabilities:\\n\", topic_word_probs)\n",
    "\n",
    "\n",
    "\n",
    "# Normalize: Converts document_topic_matrix and topic_word_matrix into probabilities, \n",
    "# making it easier to interpret the relevance of each document to each topic\n",
    "# and each term to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
